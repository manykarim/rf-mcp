name: CI

on: [push, pull_request]

jobs:
  build-and-test:
    name: Build and Test (uv)
    runs-on: ${{ matrix.os }}
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      OPENAI_MODEL: ${{ vars.OPENAI_MODEL }}
      RUN_NETWORK_TESTS: '1'  # Enable network tests in CI
    
    strategy:
      fail-fast: false
      matrix:
        os: [ ubuntu-latest, macos-latest, windows-latest ]
        python-version: [ '3.10', '3.11', '3.12' ]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup uv
        uses: astral-sh/setup-uv@v2
        with:
          version: 'latest'

      - name: Setup Python ${{ matrix.python-version }} (via uv)
        run: uv python install ${{ matrix.python-version }}

      - name: Show uv and Python versions
        run: |
          uv --version
          uv run python --version

      - name: Install dependencies (dev)
        run: uv sync --all-extras --group dev

      - name: Initialize Browser Library (rfbrowser)
        # This installs Playwright and browser engines for the Robot Framework Browser library.
        # It is safe to run even if Browser is not used; failures are ignored to avoid breaking non-UI PRs.
        run: uv run rfbrowser init
        continue-on-error: true

      - name: Install Playwright browsers
        env:
          PLAYWRIGHT_BROWSERS_PATH: '0'  # Install browsers in the virtual environment
        run: |
          uv run playwright install --with-deps chromium firefox webkit
          # Also try the python module approach in case the above doesn't work
          uv run python -m playwright install --with-deps chromium firefox webkit || true

      - name: Verify Playwright installation
        run: |
          uv run playwright --version
          echo "Checking Playwright browser locations..."
          uv run python -c "from playwright.sync_api import sync_playwright; print('Playwright import successful')"
          # Check where browsers are installed
          uv run python -c "import playwright; import os; print(f'Playwright path: {playwright.__file__}'); print(f'Browser path would be: {os.path.dirname(playwright.__file__)}')"

      - name: Prepare results directory
        run: mkdir -p results

      - name: Configure OpenAI Model with Fallback
        shell: bash
        run: |
          OPENAI_MODEL_VALUE="${{ vars.OPENAI_MODEL }}"
          if [ -z "$OPENAI_MODEL_VALUE" ]; then
            echo "⚠️ Missing OPENAI_MODEL, using fallback: gpt-5-mini"
            OPENAI_MODEL_VALUE="gpt-5-mini"
          else
            echo "✅ Using OPENAI_MODEL: $OPENAI_MODEL_VALUE"
          fi
          echo "OPENAI_MODEL=$OPENAI_MODEL_VALUE" >> $GITHUB_ENV

      - name: Run tests with coverage
        shell: bash
        env:
          # Keep runner path enabled for RequestsLibrary in CI
          ROBOTMCP_RF_RUNNER_REQUESTS: '1'
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          OPENAI_MODEL: ${{ env.OPENAI_MODEL }}
          # Ensure Playwright uses the correct browser path
          PLAYWRIGHT_BROWSERS_PATH: '0'
        run: |
          uv run coverage erase
          uv run coverage run -m pytest tests/ --junitxml=results/pytest.xml
          uv run python -c "import glob, subprocess; files = glob.glob('.coverage.*'); subprocess.run(['coverage', 'combine'], check=True) if files else print('No parallel coverage files found; skipping coverage combine.')"
          uv run coverage report
          uv run coverage html -d results/htmlcov
          uv run coverage xml -o results/coverage.xml

      - name: Upload coverage artifacts
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.os }}-py${{ matrix.python-version }}
          path: |
            results/htmlcov
            results/coverage.xml

      - name: Test Report
        uses: dorny/test-reporter@main
        if: success() || failure()
        with:
          name: xUnit Tests ${{ matrix.python-version }}
          path: results/pytest.xml
          reporter: java-junit

      - name: Build package (wheel + sdist)
        run: uv build

      - name: Upload dist artifacts
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: dist-${{ matrix.os }}-py${{ matrix.python-version }}
          path: dist/*

  e2e-tests:
    name: E2E AI Agent Tests
    runs-on: ubuntu-latest
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      OPENAI_MODEL: ${{ vars.OPENAI_MODEL }}
      USE_REAL_LLM: 'true'  # Enable real LLM testing with OpenAI API
      PLAYWRIGHT_BROWSERS_PATH: '0'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup uv
        uses: astral-sh/setup-uv@v2
        with:
          version: 'latest'

      - name: Setup Python 3.12 (via uv)
        run: uv python install 3.12

      - name: Show uv and Python versions
        run: |
          uv --version
          uv run python --version

      - name: Install dependencies (dev)
        run: uv sync --all-extras --group dev

      - name: Initialize Browser Library (rfbrowser)
        run: uv run rfbrowser init
        continue-on-error: true

      - name: Install Playwright browsers
        run: |
          uv run playwright install --with-deps chromium firefox
          uv run python -m playwright install --with-deps chromium firefox || true

      - name: Install Chrome, Firefox, and Xvfb for browser tests
        run: |
          sudo apt-get update
          sudo apt-get install -y chromium-browser firefox xvfb

      - name: Configure OpenAI Model with Fallback
        shell: bash
        run: |
          OPENAI_MODEL_VALUE="${{ vars.OPENAI_MODEL }}"
          if [ -z "$OPENAI_MODEL_VALUE" ]; then
            echo "⚠️ Missing OPENAI_MODEL, using fallback: gpt-5-mini"
            OPENAI_MODEL_VALUE="gpt-5-mini"
          else
            echo "✅ Using OPENAI_MODEL: $OPENAI_MODEL_VALUE"
          fi
          echo "OPENAI_MODEL=$OPENAI_MODEL_VALUE" >> $GITHUB_ENV

      - name: Create metrics directories
        run: |
          mkdir -p tests/e2e/metrics
          mkdir -p tests/e2e/metrics/autonomous
          mkdir -p tests/e2e/metrics/comparisons

      - name: Run E2E tool discovery tests
        run: |
          uv run pytest tests/e2e/test_agent_tool_discovery.py -v --junitxml=results/e2e-tool-discovery.xml

      - name: Run E2E autonomous agent tests (Real LLM)
        env:
          DISPLAY: ':99'
        run: |
          # Start Xvfb for browser tests
          Xvfb :99 -screen 0 1920x1080x24 &
          sleep 2
          # Run tests with display support and automatic retries for flaky LLM tests
          uv run pytest tests/e2e/test_autonomous_agents.py -v --reruns 2 --reruns-delay 5 --junitxml=results/e2e-autonomous-agents.xml
        timeout-minutes: 60
        continue-on-error: true  # Don't fail CI on LLM test failures - collect metrics instead

      - name: Upload E2E metrics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-metrics
          path: |
            tests/e2e/metrics/**/*.json

      - name: Test Report
        uses: dorny/test-reporter@main
        if: success() || failure()
        with:
          name: E2E Tests
          path: results/e2e-*.xml
          reporter: java-junit

  optional-matrix:
    name: Optional Dependency Matrix
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        include:
          - combo: slim
            extras: 'slim'
            test-command: 'uv run pytest tests/test_mcp_simple.py -q'
          - combo: web
            extras: 'web'
            test-command: 'uv run pytest -m optional_web -q'
          - combo: api
            extras: 'api'
            test-command: 'uv run pytest -m optional_api -q'
          - combo: mobile
            extras: 'mobile'
            test-command: 'uv run pytest -m optional_mobile -q'
          - combo: database
            extras: 'database'
            test-command: 'uv run pytest -m optional_database -q'
          - combo: web+api
            extras: 'web,api'
            test-command: 'uv run pytest -m optional_web_api -q'
          - combo: all
            extras: 'all'
            test-command: 'uv run pytest -m optional_all -q'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup uv
        uses: astral-sh/setup-uv@v2
        with:
          version: 'latest'

      - name: Setup Python 3.12 (via uv)
        run: uv python install 3.12

      - name: Install base dependencies
        run: uv sync --group dev

      - name: Install optional extras
        if: ${{ matrix.extras != '' }}
        run: uv pip install -e ".[${{ matrix.extras }}]"

      - name: Prepare results directory
        run: mkdir -p results

      - name: Configure OpenAI Model with Fallback
        shell: bash
        run: |
          OPENAI_MODEL_VALUE="${{ vars.OPENAI_MODEL }}"
          if [ -z "$OPENAI_MODEL_VALUE" ]; then
            OPENAI_MODEL_VALUE="gpt-5-mini"
          fi
          echo "OPENAI_MODEL=$OPENAI_MODEL_VALUE" >> $GITHUB_ENV

      - name: Initialize Browser Library (rfbrowser)
        if: ${{ contains(matrix.extras, 'web') || matrix.extras == 'all' }}
        run: uv run rfbrowser init
        continue-on-error: true

      - name: Run optional tests
        env:
          ROBOTMCP_RF_RUNNER_REQUESTS: '1'
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          OPENAI_MODEL: ${{ env.OPENAI_MODEL }}
          # Ensure Playwright uses the correct browser path
          PLAYWRIGHT_BROWSERS_PATH: '0'
          RUN_NETWORK_TESTS: '1'
        run: ${{ matrix.test-command }} --junitxml=results/pytest.xml

      - name: Test Report
        uses: dorny/test-reporter@main
        if: success() || failure()
        with:
          name: xUnit Optional Tests (${{ matrix.combo }})
          path: results/pytest.xml
          reporter: java-junit
